"""Gradio UI for the hybrid translation pipeline."""

from __future__ import annotations

from pathlib import Path
from typing import Optional

import gradio as gr

from src.pipeline import create_pipeline_from_config, load_config

PIPELINE = None


def _initialize_pipeline() -> None:
    global PIPELINE

    print("=" * 70)
    print("KH·ªûI T·∫†O ·ª®NG D·ª§NG D·ªäCH THU·∫¨T LAI (OLLAMA)")
    print("=" * 70)

    try:
        config = load_config()
        base_dir = Path(__file__).resolve().parents[2]
        ct2_dir = base_dir / config["stage1_model_dir"]

        if not ct2_dir.exists() or not (ct2_dir / "model.bin").exists():
            print(f"‚ùå M√¥ h√¨nh Giai ƒëo·∫°n 1 ch∆∞a ƒë∆∞·ª£c chu·∫©n b·ªã: {ct2_dir}")
            print("\n" + "=" * 70)
            print("‚ö†Ô∏è  M√î H√åNH GIAI ƒêO·∫†N 1 CH∆ØA S·∫¥N S√ÄNG")
            print("=" * 70)
            print("H√£y ch·∫°y l·ªánh sau ƒë·ªÉ t·∫£i xu·ªëng v√† chu·∫©n b·ªã m√¥ h√¨nh:\n")
            print("    python scripts/setup_models.py\n")
            print("=" * 70)
            PIPELINE = None
            return

        print("\nüöÄ ƒêang t·∫£i c√°c m√¥ h√¨nh...")
        PIPELINE = create_pipeline_from_config()
        print("\n‚úÖ ·ª®ng d·ª•ng ƒë√£ s·∫µn s√†ng!")

    except Exception as exc:  # pragma: no cover - user runtime feedback
        print(f"\n‚ùå L·ªói kh·ªüi t·∫°o:\n{exc}")
        PIPELINE = None


def translate_interface(input_text: str):
    if PIPELINE is None:
        error_msg = (
            "‚ùå Pipeline ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.\n\n"
            "H√£y:\n"
            "1. Ch·∫°y: python scripts/setup_models.py\n"
            "2. C√†i ƒë·∫∑t Ollama: https://ollama.ai/download\n"
            "3. Ch·∫°y: ollama pull llama3.2:3b\n"
            "4. Kh·ªüi ƒë·ªông l·∫°i ·ª©ng d·ª•ng"
        )
        return "", "", "", error_msg

    if not input_text or not input_text.strip():
        return "", "", "", "‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ti·∫øng Anh."

    try:
        result = PIPELINE.translate_and_refine(input_text)

        if "error" in result:
            return "", "", "", f"‚ùå L·ªói: {result['error']}"

        raw = result["raw_translation"]
        refined = result["refined_translation"]
        time_s1 = result["time_stage1_sec"]
        time_s2 = result["time_stage2_sec"]
        time_info = (
            "‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω:\n"
            f"  ‚Ä¢ Giai ƒëo·∫°n 1 (D·ªãch th√¥): {time_s1:.3f}s\n"
            f"  ‚Ä¢ Giai ƒëo·∫°n 2 (Tinh ch·ªânh qua Ollama): {time_s2:.3f}s\n"
            f"  ‚Ä¢ T·ªïng c·ªông: {time_s1 + time_s2:.3f}s\n\n"
            f"üìä ƒê·ªô d√†i:\n"
            f"  ‚Ä¢ Input: {len(input_text)} k√Ω t·ª±\n"
            f"  ‚Ä¢ D·ªãch th√¥: {len(raw)} k√Ω t·ª±\n"
            f"  ‚Ä¢ Tinh ch·ªânh: {len(refined)} k√Ω t·ª±"
        )

        return raw, refined, time_info, ""

    except Exception as exc:  # pragma: no cover - runtime safeguard
        return "", "", "", f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {exc}"


def create_app() -> gr.Blocks:
    custom_css = """
    #output_refined {
        background-color: #e8f5e9 !important;
    }
    #output_raw {
        background-color: #fff3e0 !important;
    }
    """

    config = load_config()

    with gr.Blocks(
        title="D·ªãch thu·∫≠t Lai Anh-Vi·ªát (Ollama)",
        css=custom_css,
    ) as app:
        gr.Markdown(
            """
            # üîÑ C√¥ng c·ª• D·ªãch thu·∫≠t Lai Anh-Vi·ªát (Ollama)

            **Pipeline hai giai ƒëo·∫°n "Translate-and-Refine"**

            - **Giai ƒëo·∫°n 1:** D·ªãch th√¥ t·ªëc ƒë·ªô cao (Helsinki-NLP OPUS-MT)
            - **Giai ƒëo·∫°n 2:** Tinh ch·ªânh ng·ªØ c·∫£nh b·∫±ng Ollama LLM

            ---
            """
        )

        if PIPELINE is None:
            gr.Markdown(
                """
                ## ‚ö†Ô∏è C·∫¢NH B√ÅO: M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c chu·∫©n b·ªã

                H√£y l√†m theo c√°c b∆∞·ªõc sau:

                1. **Chu·∫©n b·ªã m√¥ h√¨nh Stage 1:**
                   ```
                   python scripts/setup_models.py
                   ```

                2. **C√†i ƒë·∫∑t Ollama:**
                   - T·∫£i t·ª´: https://ollama.ai/download
                   - C√†i ƒë·∫∑t v√† kh·ªüi ƒë·ªông Ollama

                3. **T·∫£i m√¥ h√¨nh LLM:**
                   ```
                   ollama pull llama3.2:3b
                   ```

                4. **Kh·ªüi ƒë·ªông l·∫°i ·ª©ng d·ª•ng**
                """
            )

        with gr.Row():
            with gr.Column(scale=1):
                input_box = gr.Textbox(
                    label="üìù VƒÉn b·∫£n ti·∫øng Anh",
                    placeholder="Nh·∫≠p vƒÉn b·∫£n ti·∫øng Anh c·∫ßn d·ªãch...",
                    lines=10,
                    max_lines=20,
                )

                translate_btn = gr.Button(
                    "üöÄ D·ªãch thu·∫≠t",
                    variant="primary",
                    size="lg",
                )

                gr.Examples(
                    examples=[
                        "Hello world! This is a test of the translation system.",
                        "The enterprise solution must be robust and scalable.",
                        "Artificial intelligence is transforming the way we work and live.",
                        "Climate change poses significant challenges for future generations.",
                    ],
                    inputs=input_box,
                    label="üìö V√≠ d·ª•",
                )

            with gr.Column(scale=1):
                output_raw = gr.Textbox(
                    label="üîÑ B·∫£n d·ªãch th√¥ (Giai ƒëo·∫°n 1)",
                    lines=15,
                    max_lines=30,
                    interactive=False,
                    elem_id="output_raw",
                    show_copy_button=True,
                )

                output_refined = gr.Textbox(
                    label="‚ú® B·∫£n d·ªãch ƒë√£ tinh ch·ªânh (Giai ƒëo·∫°n 2 - Ollama)",
                    lines=15,
                    max_lines=30,
                    interactive=False,
                    elem_id="output_refined",
                    show_copy_button=True,
                )

                time_info = gr.Textbox(
                    label="‚è±Ô∏è Th√¥ng tin th·ªùi gian",
                    lines=4,
                    interactive=False,
                )

                error_box = gr.Textbox(
                    label="‚ùå L·ªói (n·∫øu c√≥)",
                    lines=2,
                    interactive=False,
                    visible=True,
                )

        gr.Markdown(
            f"""
            ---
            ### ‚öôÔ∏è C·∫•u h√¨nh hi·ªán t·∫°i
            - **Ollama Model:** {config.get('ollama_model', 'N/A')} (ƒêi·ªÅu ch·ªânh trong `src/config/settings.json`)
            - **Ollama Host:** {config.get('ollama_host', 'N/A')}
            - **Temperature:** {config.get('temperature', 'N/A')}

            ### üí° Thay ƒë·ªïi m√¥ h√¨nh Ollama

            M·ªü `src/config/settings.json` v√† s·ª≠a:
            ```json
            "ollama_model": "llama3.2:3b"
            ```

            **C√°c m√¥ h√¨nh khuy·∫øn ngh·ªã:**
            - `llama3.2:3b` - Nh·ªè, nhanh (~2GB)
            - `llama3:8b` - C√¢n b·∫±ng (~5GB)
            - `qwen2.5:7b` - Ch·∫•t l∆∞·ª£ng cao (~5GB)

            Sau khi ƒë·ªïi, ch·∫°y: `ollama pull <t√™n-m√¥-h√¨nh>`
            """
        )

        translate_btn.click(
            fn=translate_interface,
            inputs=[input_box],
            outputs=[output_raw, output_refined, time_info, error_box],
        )

    return app


def main() -> None:
    _initialize_pipeline()

    app = create_app()

    print("\n" + "=" * 70)
    print("üåê KH·ªûI ƒê·ªòNG GIAO DI·ªÜN WEB (OLLAMA)")
    print("=" * 70)
    print("·ª®ng d·ª•ng s·∫Ω m·ªü trong tr√¨nh duy·ªát c·ªßa b·∫°n...")
    print("Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng.")
    print("=" * 70 + "\n")

    app.launch(
        server_name="127.0.0.1",
        server_port=None,
        share=False,
        show_error=True,
    )


if __name__ == "__main__":
    main()
