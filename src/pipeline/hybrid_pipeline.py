"""Hybrid translation pipeline powered by CTranslate2 and Ollama."""

from __future__ import annotations

import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, Optional

import ctranslate2
import requests
import transformers

REPO_ROOT = Path(__file__).resolve().parents[2]
CONFIG_DIR = REPO_ROOT / "src" / "config"


class HybridTranslationPipelineOllama:
    """Translate-and-refine pipeline that uses Ollama for refinement."""

    def __init__(
        self,
        ct2_model_dir: str,
        hf_model_name: str,
        ollama_model: str = "llama3:8b",
        ollama_host: str = "http://localhost:11434",
        temperature: float = 0.2,
        beam_size: int = 2,
        use_ollama_only: bool = False,  # Chá»‰ dÃ¹ng Ollama, bá» qua Stage 1
    ) -> None:
        print("ğŸš€ Äang khá»Ÿi táº¡o Pipeline Dá»‹ch thuáº­t Lai (Ollama)...")

        self.ollama_model = ollama_model
        self.ollama_host = ollama_host
        self.temperature = temperature
        self.beam_size = beam_size
        self.use_ollama_only = use_ollama_only
        self.translator: Optional[ctranslate2.Translator] = None
        self.tokenizer: Optional[transformers.PreTrainedTokenizer] = None

        if not use_ollama_only:
            self._init_stage1(ct2_model_dir, hf_model_name)
        else:
            print("\nâš¡ Cháº¿ Ä‘á»™: Chá»‰ dÃ¹ng Ollama (bá» qua CTranslate2)")
            
        self._check_ollama()

        print("âœ… Pipeline Ä‘Ã£ sáºµn sÃ ng!")

    def _get_lang_token(self, lang_code: str) -> Optional[str]:
        """Return tokenizer token string for a given language code."""
        tokenizer = getattr(self, "tokenizer", None)
        if tokenizer is None:
            return None

        if getattr(self, "model_type", None) == "m2m100":
            # Try lang_code_to_token first
            token = getattr(tokenizer, "lang_code_to_token", {}).get(lang_code)
            if token:
                return token
            if hasattr(tokenizer, "get_lang_id"):
                try:
                    token_id = tokenizer.get_lang_id(lang_code)
                    return tokenizer.convert_ids_to_tokens([token_id])[0]
                except Exception:
                    return None

        return None

    def _build_source_tokens(self, text: str) -> list[str]:
        """Convert raw text into token list compatible with translator."""
        assert self.tokenizer is not None

        if getattr(self, "model_type", None) == "m2m100":
            self.tokenizer.src_lang = "en"
            encoded_ids = self.tokenizer.encode(text, add_special_tokens=True)
            return self.tokenizer.convert_ids_to_tokens(encoded_ids)

        encoded_ids = self.tokenizer.encode(f">>vie<< {text}")
        return self.tokenizer.convert_ids_to_tokens(encoded_ids)

    def _init_stage1(self, ct2_model_dir: str, hf_model_name: str) -> None:
        print("\nğŸ“¦ Giai Ä‘oáº¡n 1: Táº£i mÃ´ hÃ¬nh CTranslate2")
        print(f"   ÄÆ°á»ng dáº«n: {ct2_model_dir}")

        if not os.path.exists(ct2_model_dir):
            raise FileNotFoundError(
                f"âŒ ThÆ° má»¥c mÃ´ hÃ¬nh CTranslate2 '{ct2_model_dir}' khÃ´ng tÃ¬m tháº¥y.\n"
                "   HÃ£y cháº¡y: python scripts/setup_models.py"
            )

        try:
            self.translator = ctranslate2.Translator(ct2_model_dir, device="cuda")
            print("   âœ… ÄÃ£ táº£i trÃªn CUDA (GPU)")
        except Exception as exc:  # pragma: no cover - hardware dependent
            print(f"   âš ï¸  KhÃ´ng thá»ƒ táº£i trÃªn CUDA: {exc}")
            print("   ğŸ”„ Äang chuyá»ƒn sang CPU...")
            self.translator = ctranslate2.Translator(ct2_model_dir, device="cpu")
            print("   âœ… ÄÃ£ táº£i trÃªn CPU")

        print(f"   Äang táº£i tokenizer: {hf_model_name}")
        
        # Kiá»ƒm tra loáº¡i model Ä‘á»ƒ load Ä‘Ãºng tokenizer
        if "m2m100" in hf_model_name.lower():
            self.tokenizer = transformers.M2M100Tokenizer.from_pretrained(hf_model_name)
            self.model_type = "m2m100"
            self.tokenizer.src_lang = "en"
            self.tokenizer.tgt_lang = "vi"
            print("   ğŸ“ Loáº¡i: M2M100 (Facebook)")
        else:
            self.tokenizer = transformers.AutoTokenizer.from_pretrained(hf_model_name)
            self.model_type = "opus"
            print("   ğŸ“ Loáº¡i: OPUS-MT (Helsinki)")
            
            # Kiá»ƒm tra vÃ  thiáº¿t láº­p target language cho OPUS-MT
            if hasattr(self.tokenizer, 'tgt_lang'):
                self.tokenizer.tgt_lang = 'vi'
                print("   ğŸ¯ Target language: vi (Vietnamese)")
            if hasattr(self.tokenizer, 'src_lang'):
                self.tokenizer.src_lang = 'en'
                print("   ğŸ¯ Source language: en (English)")
            print("   ğŸ“ Loáº¡i: OPUS-MT (Helsinki)")
            
        print("   âœ… Tokenizer Ä‘Ã£ sáºµn sÃ ng")

    def _check_ollama(self) -> None:
        print("\nğŸ“¦ Giai Ä‘oáº¡n 2: Kiá»ƒm tra Ollama")
        print(f"   Host: {self.ollama_host}")
        print(f"   Model: {self.ollama_model}")

        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
            if response.status_code != 200:
                raise ConnectionError("Ollama server khÃ´ng pháº£n há»“i")

            models = response.json().get("models", [])
            model_names = [model["name"] for model in models]

            if self.ollama_model not in model_names:
                print(f"   âš ï¸  MÃ´ hÃ¬nh '{self.ollama_model}' chÆ°a Ä‘Æ°á»£c táº£i")
                print("   ğŸ“¥ Äang táº£i mÃ´ hÃ¬nh... (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)")
                self._pull_ollama_model()
            else:
                print("   âœ… MÃ´ hÃ¬nh Ä‘Ã£ sáºµn sÃ ng")

        except requests.exceptions.ConnectionError as exc:
            raise ConnectionError(
                f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n Ollama táº¡i {self.ollama_host}\n\n"
                "HÃ£y Ä‘áº£m báº£o Ollama Ä‘ang cháº¡y:\n"
                "1. Táº£i Ollama: https://ollama.ai/download\n"
                "2. CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng Ollama\n"
                f"3. Cháº¡y: ollama pull {self.ollama_model}\n"
                "4. Cháº¡y láº¡i á»©ng dá»¥ng"
            ) from exc

    def _pull_ollama_model(self) -> None:
        try:
            response = requests.post(
                f"{self.ollama_host}/api/pull",
                json={"name": self.ollama_model},
                stream=True,
                timeout=600,
            )

            for line in response.iter_lines():
                if not line:
                    continue
                data = json.loads(line)
                status = data.get("status", "")
                if "pulling" in status.lower():
                    print(f"   ğŸ“¥ {status}")

            print("   âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i")

        except Exception as exc:
            raise RuntimeError(
                f"âŒ Lá»—i khi táº£i mÃ´ hÃ¬nh Ollama:\n{exc}\n\n"
                f"HÃ£y táº£i thá»§ cÃ´ng: ollama pull {self.ollama_model}"
            ) from exc

    def _split_text(self, text: str, max_tokens: int = 400) -> list[str]:
        """Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n nhá» hÆ¡n max_tokens."""
        assert self.tokenizer is not None
        
        print(f"\n   ğŸ” DEBUG: Äang chia vÄƒn báº£n ({len(text)} kÃ½ tá»±)...")
        
        # Chia theo cÃ¢u - regex phá»©c táº¡p hÆ¡n Ä‘á»ƒ xá»­ lÃ½ footnote, sá»‘, v.v.
        import re
        # TÃ¬m dáº¥u káº¿t thÃºc cÃ¢u: . ! ? theo sau bá»Ÿi space vÃ  chá»¯ hoa HOáº¶C cuá»‘i vÄƒn báº£n
        # NhÆ°ng khÃ´ng pháº£i sá»‘ tháº­p phÃ¢n (1.5) hoáº·c viáº¿t táº¯t phá»• biáº¿n
        sentence_ends = re.finditer(r'([.!?]+)\s+(?=[A-Z])|([.!?]+)$', text)
        
        positions = [0]
        for match in sentence_ends:
            end_pos = match.end()
            if end_pos < len(text):
                positions.append(end_pos)
        positions.append(len(text))
        
        # Táº¡o list cÃ¡c cÃ¢u
        sentences = []
        for i in range(len(positions) - 1):
            sentence = text[positions[i]:positions[i+1]].strip()
            if sentence:
                sentences.append(sentence)
        
        print(f"   ğŸ“ Chia Ä‘Æ°á»£c {len(sentences)} cÃ¢u tá»« regex")
        
        # Náº¿u khÃ´ng chia Ä‘Æ°á»£c cÃ¢u, chia theo Ä‘oáº¡n vÄƒn
        if len(sentences) <= 1:
            paragraphs = text.split('\n')
            sentences = [p.strip() for p in paragraphs if p.strip()]
            print(f"   ğŸ“ Fallback: Chia theo Ä‘oáº¡n vÄƒn â†’ {len(sentences)} Ä‘oáº¡n")
        
        # GhÃ©p cÃ¢u thÃ nh chunks
        chunks = []
        current_chunk = ""
        sentences_in_chunk = 0
        
        print(f"   ğŸ”¨ Báº¯t Ä‘áº§u ghÃ©p {len(sentences)} cÃ¢u thÃ nh chunks (max {max_tokens} tokens)...")
        
        for idx, sentence in enumerate(sentences, 1):
            if not sentence:
                continue
            
            # Thá»­ thÃªm cÃ¢u vÃ o chunk hiá»‡n táº¡i
            test_chunk = current_chunk + ("\n" if current_chunk and '\n' in text else " " if current_chunk else "") + sentence
            test_tokens = self._build_source_tokens(test_chunk)
            
            if len(test_tokens) > max_tokens:
                # Chunk hiá»‡n táº¡i Ä‘Ã£ Ä‘á»§ lá»›n, lÆ°u láº¡i
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    print(f"      â†’ Chunk {len(chunks)}: {sentences_in_chunk} cÃ¢u, {len(current_chunk)} kÃ½ tá»±")
                    current_chunk = sentence
                    sentences_in_chunk = 1
                else:
                    # CÃ¢u Ä‘Æ¡n quÃ¡ dÃ i, buá»™c pháº£i cáº¯t theo tá»«
                    print(f"      âš ï¸ CÃ¢u {idx} quÃ¡ dÃ i ({len(sentence)} kÃ½ tá»±), chia nhá» hÆ¡n...")
                    words = sentence.split()
                    temp_chunk = ""
                    for word in words:
                        test_word = temp_chunk + (" " if temp_chunk else "") + word
                        if len(self._build_source_tokens(test_word)) > max_tokens:
                            if temp_chunk:
                                chunks.append(temp_chunk.strip())
                                print(f"      â†’ Chunk {len(chunks)}: tá»« bá»‹ cáº¯t, {len(temp_chunk)} kÃ½ tá»±")
                                temp_chunk = word
                            else:
                                # Tá»« Ä‘Æ¡n quÃ¡ dÃ i, thÃªm vÃ o luÃ´n
                                chunks.append(word)
                                print(f"      â†’ Chunk {len(chunks)}: 1 tá»« dÃ i, {len(word)} kÃ½ tá»±")
                                temp_chunk = ""
                        else:
                            temp_chunk = test_word
                    
                    if temp_chunk:
                        current_chunk = temp_chunk
                        sentences_in_chunk = 1
            else:
                current_chunk = test_chunk
                sentences_in_chunk += 1
        
        # ThÃªm chunk cuá»‘i
        if current_chunk and current_chunk.strip():
            chunks.append(current_chunk.strip())
            print(f"      â†’ Chunk {len(chunks)} (cuá»‘i): {sentences_in_chunk} cÃ¢u, {len(current_chunk)} kÃ½ tá»±")
        
        total_chunk_chars = sum(len(c) for c in chunks)
        print(f"   âœ… Tá»•ng káº¿t chia vÄƒn báº£n:")
        print(f"      - Input: {len(text)} kÃ½ tá»±")
        print(f"      - Output: {len(chunks)} chunks, {total_chunk_chars} kÃ½ tá»±")
        print(f"      - Máº¥t: {len(text) - total_chunk_chars} kÃ½ tá»± ({(len(text)-total_chunk_chars)/len(text)*100:.1f}%)")
        
        return chunks if chunks else [text]

    def _translate_stage1(self, source_text: str) -> str:
        assert self.tokenizer is not None
        assert self.translator is not None

        # Kiá»ƒm tra Ä‘á»™ dÃ i vÃ  chia nhá» náº¿u cáº§n
        test_tokens = self._build_source_tokens(source_text)
        
        # Náº¿u vÄƒn báº£n quÃ¡ dÃ i, chia nhá»
        if len(test_tokens) > 500:
            print(f"\n   âš ï¸  VÄƒn báº£n dÃ i ({len(test_tokens)} tokens, {len(source_text)} kÃ½ tá»±)")
            print(f"   ğŸ“‹ Äang chia nhá» vÄƒn báº£n...")
            chunks = self._split_text(source_text)
            print(f"   ğŸ“¦ ÄÃ£ chia thÃ nh {len(chunks)} Ä‘oáº¡n")
            
            # Kiá»ƒm tra tá»•ng Ä‘á»™ dÃ i chunks
            total_chunk_length = sum(len(c) for c in chunks)
            print(f"   ğŸ“ Tá»•ng Ä‘á»™ dÃ i chunks: {total_chunk_length} kÃ½ tá»± (gá»‘c: {len(source_text)} kÃ½ tá»±)")
            
            if total_chunk_length < len(source_text) - 10:
                print(f"   âš ï¸  Cáº¢NH BÃO: Máº¥t {len(source_text) - total_chunk_length} kÃ½ tá»± khi chia!")
            
            translated_chunks = []
            
            for i, chunk in enumerate(chunks, 1):
                chunk_preview = chunk[:50] + "..." if len(chunk) > 50 else chunk
                print(f"\n   ğŸ”„ Äoáº¡n {i}/{len(chunks)}: {len(chunk)} kÃ½ tá»±")
                print(f"      Preview: {chunk_preview}")
                
                # Xá»­ lÃ½ token theo loáº¡i model
                chunk_tokens = self._build_source_tokens(chunk)
                print(f"      First 5 tokens: {chunk_tokens[:5]}")

                print(f"      Tokens: {len(chunk_tokens)}")
                
                # TÃ­nh toÃ¡n max_decoding_length dá»±a trÃªn Ä‘á»™ dÃ i input
                # Tiáº¿ng Viá»‡t thÆ°á»ng dÃ i hÆ¡n tiáº¿ng Anh 1.2-1.5 láº§n
                estimated_output_length = int(len(chunk_tokens) * 1.5) + 50
                max_length = max(min(estimated_output_length, 2048), 512)
                
                print(f"      Max output length: {max_length} tokens")
                
                # Debug: Hiá»ƒn thá»‹ vÃ i token Ä‘áº§u Ä‘á»ƒ kiá»ƒm tra
                print(f"      Input tokens (first 10): {chunk_tokens[:10]}")
                
                # Chuáº©n bá»‹ target prefix náº¿u cÃ³ token tiáº¿ng Viá»‡t
                target_prefix = None
                if hasattr(self, 'model_type') and self.model_type == "m2m100":
                    vi_token = self._get_lang_token("vi")
                    if vi_token:
                        target_prefix = [[vi_token]]
                        print(f"      Target prefix (M2M100): {target_prefix}")
                
                translate_kwargs = {
                    "beam_size": self.beam_size,
                    "max_decoding_length": max_length,
                    "min_decoding_length": int(len(chunk_tokens) * 0.8),
                    "repetition_penalty": 1.2,
                    "no_repeat_ngram_size": 3,
                    "return_scores": False,
                }
                if target_prefix:
                    translate_kwargs["target_prefix"] = target_prefix

                results = self.translator.translate_batch(
                    [chunk_tokens],
                    **translate_kwargs,
                )
                
                target_tokens = results[0].hypotheses[0]
                print(f"      Output tokens: {len(target_tokens)}")
                print(f"      Output tokens (first 20): {target_tokens[:20]}")
                
                target_text = self.tokenizer.decode(
                    self.tokenizer.convert_tokens_to_ids(target_tokens),
                    skip_special_tokens=True,
                )
                
                # Kiá»ƒm tra xem cÃ³ pháº£i tiáº¿ng Viá»‡t khÃ´ng
                vietnamese_chars = sum(1 for c in target_text if ord(c) > 127)
                print(f"      Vietnamese chars: {vietnamese_chars}/{len(target_text)}")
                
                if not target_text or not target_text.strip():
                    print(f"      âš ï¸  Cáº¢NH BÃO: Äoáº¡n {i} dá»‹ch ra rá»—ng!")
                else:
                    translated_chunks.append(target_text.strip())
                    translation_preview = target_text[:50] + "..." if len(target_text) > 50 else target_text
                    ratio = len(target_text) / len(chunk) * 100
                    print(f"      âœ“ Output: {len(target_text)} kÃ½ tá»± ({ratio:.1f}% input)")
                    print(f"      Preview: {translation_preview}")
                    
                    # Cáº£nh bÃ¡o náº¿u output khÃ´ng pháº£i tiáº¿ng Viá»‡t
                    if vietnamese_chars < len(target_text) * 0.1:
                        print(f"      âš ï¸  Cáº¢NH BÃO: Báº£n dá»‹ch cÃ³ váº» KHÃ”NG PHáº¢I tiáº¿ng Viá»‡t!")
                    
                    # Cáº£nh bÃ¡o náº¿u output quÃ¡ ngáº¯n
                    if ratio < 50:
                        print(f"      âš ï¸  Cáº¢NH BÃO: Báº£n dá»‹ch cÃ³ váº» ngáº¯n báº¥t thÆ°á»ng!")
            
            full_translation = " ".join(translated_chunks)
            print(f"\n   âœ… Tá»•ng káº¿t:")
            print(f"      - Input: {len(source_text)} kÃ½ tá»±")
            print(f"      - Chunks: {len(chunks)} Ä‘oáº¡n")
            print(f"      - Output: {len(full_translation)} kÃ½ tá»±")
            print(f"      - Tá»· lá»‡: {len(full_translation)/len(source_text)*100:.1f}%")
            return full_translation
        
        # VÄƒn báº£n ngáº¯n, dá»‹ch trá»±c tiáº¿p
        source_tokens = self._build_source_tokens(source_text)
        
        estimated_output_length = int(len(test_tokens) * 1.5) + 50
        max_length = max(min(estimated_output_length, 2048), 512)
        
        # Chuáº©n bá»‹ target_prefix Ä‘á»ƒ force tiáº¿ng Viá»‡t
        target_prefix = None
        if hasattr(self, 'model_type') and self.model_type == "m2m100":
            vi_token = self._get_lang_token("vi")
            if vi_token:
                target_prefix = [[vi_token]]
        elif hasattr(self, 'model_type') and self.model_type == "opus":
            if hasattr(self.tokenizer, 'lang_code_to_id') and 'vi' in self.tokenizer.lang_code_to_id:
                vi_token_id = self.tokenizer.lang_code_to_id['vi']
                target_prefix = [[self.tokenizer.convert_ids_to_tokens([vi_token_id])[0]]]

        translate_kwargs = {
            "beam_size": self.beam_size,
            "max_decoding_length": max_length,
            "min_decoding_length": int(len(test_tokens) * 0.8),
            "repetition_penalty": 1.2,
            "no_repeat_ngram_size": 3,
        }
        if target_prefix:
            translate_kwargs["target_prefix"] = target_prefix

        results = self.translator.translate_batch(
            [source_tokens],
            **translate_kwargs,
        )

        target_tokens = results[0].hypotheses[0]
        target_text = self.tokenizer.decode(
            self.tokenizer.convert_tokens_to_ids(target_tokens),
            skip_special_tokens=True,
        )

        return target_text

    def _refine_stage2(self, source_text: str, raw_translation: str) -> str:
        prompt = f"""Cáº£i thiá»‡n báº£n dá»‹ch tiáº¿ng Viá»‡t sau Ä‘á»ƒ nghe tá»± nhiÃªn vÃ  mÆ°á»£t mÃ  hÆ¡n.

YÃŠU Cáº¦U QUAN TRá»ŒNG:
1. Giá»¯ NGUYÃŠN cÃ¡c danh tá»« riÃªng, tÃªn cÃ´ng ty, tÃªn sáº£n pháº©m, thuáº­t ngá»¯ chuyÃªn ngÃ nh báº±ng tiáº¿ng Anh
2. Sau cÃ¡c thuáº­t ngá»¯ tiáº¿ng Anh, thÃªm báº£n dá»‹ch táº¡m trong ngoáº·c Ä‘Æ¡n. VÃ­ dá»¥: "GeoCity (á»©ng dá»¥ng Ä‘á»‹a lÃ½)", "Computer Club (CÃ¢u láº¡c bá»™ MÃ¡y tÃ­nh)"
3. LÃ m cho vÄƒn báº£n nghe tá»± nhiÃªn, mÆ°á»£t mÃ  nhÆ° ngÆ°á»i Viá»‡t nÃ³i
4. Giá»¯ nguyÃªn Ã½ nghÄ©a vÃ  phong cÃ¡ch cá»§a vÄƒn báº£n gá»‘c

VÄƒn báº£n tiáº¿ng Anh gá»‘c: {source_text}

Báº£n dá»‹ch hiá»‡n táº¡i: {raw_translation}

Báº£n dá»‹ch cáº£i tiáº¿n:"""

        estimated_tokens = len(raw_translation) // 2 + 500
        max_tokens = min(max(estimated_tokens, 2048), 16384)

        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "system": (
                        "Báº¡n lÃ  má»™t biÃªn táº­p viÃªn chuyÃªn nghiá»‡p vá» dá»‹ch thuáº­t tiáº¿ng Viá»‡t. "
                        "Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  cáº£i thiá»‡n báº£n dá»‹ch Ä‘á»ƒ nghe tá»± nhiÃªn, mÆ°á»£t mÃ  nhÆ° ngÆ°á»i Viá»‡t báº£n xá»©. "
                        "LuÃ´n GIá»® NGUYÃŠN cÃ¡c thuáº­t ngá»¯ tiáº¿ng Anh (tÃªn riÃªng, tÃªn cÃ´ng ty, sáº£n pháº©m, thuáº­t ngá»¯ ká»¹ thuáº­t) "
                        "vÃ  thÃªm báº£n dá»‹ch táº¡m trong ngoáº·c Ä‘Æ¡n ngay sau Ä‘Ã³. "
                        "CHá»ˆ tráº£ vá» vÄƒn báº£n tiáº¿ng Viá»‡t Ä‘Ã£ Ä‘Æ°á»£c cáº£i thiá»‡n, KHÃ”NG giáº£i thÃ­ch hay bÃ¬nh luáº­n gÃ¬ thÃªm."
                    ),
                    "stream": False,
                    "options": {
                        "temperature": self.temperature,
                        "num_predict": max_tokens,
                        "stop": ["English:", "Original:", "Source:", "Note:", "VÄƒn báº£n tiáº¿ng Anh:", "Báº£n dá»‹ch hiá»‡n táº¡i:"],
                    },
                },
                timeout=600,
            )

            if response.status_code != 200:
                raise RuntimeError(f"Ollama API lá»—i: {response.status_code}")

            result = response.json()
            refined_text = result.get("response", "").strip()

            # Loáº¡i bá» cÃ¡c prefix khÃ´ng mong muá»‘n
            prefixes_to_remove = [
                "Báº£n dá»‹ch cáº£i tiáº¿n:",
                "Báº£n dá»‹ch:",
                "Improved Vietnamese:",
                "Improved:",
                "Translation:",
                "Vietnamese:",
            ]
            for prefix in prefixes_to_remove:
                if refined_text.startswith(prefix):
                    refined_text = refined_text[len(prefix):].strip()

            return refined_text if refined_text else raw_translation

        except requests.exceptions.Timeout as exc:
            raise TimeoutError("Ollama API timeout. Thá»­ láº¡i hoáº·c tÄƒng timeout.") from exc
        except Exception as exc:
            raise RuntimeError(f"Lá»—i khi gá»i Ollama: {exc}") from exc

    def _translate_with_ollama_only(self, source_text: str) -> str:
        """Dá»‹ch trá»±c tiáº¿p báº±ng Ollama (cháº­m hÆ¡n nhÆ°ng cháº¥t lÆ°á»£ng tá»‘t)."""
        prompt = f"""Dá»‹ch vÄƒn báº£n tiáº¿ng Anh sau sang tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn vÃ  mÆ°á»£t mÃ .

YÃŠU Cáº¦U:
1. GIá»® NGUYÃŠN cÃ¡c danh tá»« riÃªng, tÃªn cÃ´ng ty, sáº£n pháº©m, thuáº­t ngá»¯ chuyÃªn ngÃ nh báº±ng tiáº¿ng Anh
2. Sau thuáº­t ngá»¯ tiáº¿ng Anh, thÃªm báº£n dá»‹ch táº¡m trong ngoáº·c Ä‘Æ¡n
3. Dá»‹ch tá»± nhiÃªn nhÆ° ngÆ°á»i Viá»‡t nÃ³i, khÃ´ng dá»‹ch sÃ¡t tá»«ng tá»«
4. KHÃ”NG láº·p láº¡i ná»™i dung, má»—i Ã½ chá»‰ dá»‹ch Má»˜T Láº¦N

VÄƒn báº£n tiáº¿ng Anh:
{source_text}

Báº£n dá»‹ch tiáº¿ng Viá»‡t:"""

        estimated_tokens = len(source_text) // 2 + 500
        max_tokens = min(max(estimated_tokens, 2048), 16384)

        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "system": (
                        "Báº¡n lÃ  má»™t dá»‹ch giáº£ chuyÃªn nghiá»‡p Anh-Viá»‡t. "
                        "Dá»‹ch chÃ­nh xÃ¡c, tá»± nhiÃªn, khÃ´ng láº·p láº¡i ná»™i dung. "
                        "Giá»¯ nguyÃªn thuáº­t ngá»¯ tiáº¿ng Anh vÃ  thÃªm báº£n dá»‹ch trong ngoáº·c. "
                        "CHá»ˆ tráº£ vá» báº£n dá»‹ch, KHÃ”NG giáº£i thÃ­ch."
                    ),
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "num_predict": max_tokens,
                        "repeat_penalty": 1.2,
                        "stop": ["English:", "VÄƒn báº£n tiáº¿ng Anh:", "\n\nEnglish", "\n\nVÄƒn báº£n"],
                    },
                },
                timeout=600,
            )

            if response.status_code != 200:
                raise RuntimeError(f"Ollama API lá»—i: {response.status_code}")

            result = response.json()
            translation = result.get("response", "").strip()

            # Loáº¡i bá» prefix
            prefixes = ["Báº£n dá»‹ch tiáº¿ng Viá»‡t:", "Báº£n dá»‹ch:", "Translation:"]
            for prefix in prefixes:
                if translation.startswith(prefix):
                    translation = translation[len(prefix):].strip()

            return translation

        except Exception as exc:
            raise RuntimeError(f"Lá»—i khi gá»i Ollama: {exc}") from exc

    def translate_and_refine(self, text_to_translate: str) -> Dict:
        if not text_to_translate or not text_to_translate.strip():
            return {
                "source": "",
                "raw_translation": "",
                "refined_translation": "",
                "time_stage1_sec": 0.0,
                "time_stage2_sec": 0.0,
                "error": "VÄƒn báº£n Ä‘áº§u vÃ o trá»‘ng",
            }

        # Náº¿u chá»‰ dÃ¹ng Ollama, dá»‹ch trá»±c tiáº¿p
        if self.use_ollama_only:
            start = time.time()
            try:
                translation = self._translate_with_ollama_only(text_to_translate)
                end = time.time()
                return {
                    "source": text_to_translate,
                    "raw_translation": translation,
                    "refined_translation": translation,
                    "time_stage1_sec": end - start,
                    "time_stage2_sec": 0.0,
                }
            except Exception as exc:
                return {
                    "source": text_to_translate,
                    "raw_translation": "",
                    "refined_translation": "",
                    "time_stage1_sec": 0.0,
                    "time_stage2_sec": 0.0,
                    "error": f"Lá»—i Ollama: {exc}",
                }

        # Pipeline bÃ¬nh thÆ°á»ng (2 giai Ä‘oáº¡n)
        start_s1 = time.time()
        try:
            raw_translation = self._translate_stage1(text_to_translate)
        except Exception as exc:
            return {
                "source": text_to_translate,
                "raw_translation": "",
                "refined_translation": "",
                "time_stage1_sec": 0.0,
                "time_stage2_sec": 0.0,
                "error": f"Lá»—i Giai Ä‘oáº¡n 1: {exc}",
            }
        end_s1 = time.time()

        start_s2 = time.time()
        try:
            refined_translation = self._refine_stage2(text_to_translate, raw_translation)
        except Exception as exc:
            return {
                "source": text_to_translate,
                "raw_translation": raw_translation,
                "refined_translation": "",
                "time_stage1_sec": end_s1 - start_s1,
                "time_stage2_sec": 0.0,
                "error": f"Lá»—i Giai Ä‘oáº¡n 2: {exc}",
            }
        end_s2 = time.time()

        return {
            "source": text_to_translate,
            "raw_translation": raw_translation,
            "refined_translation": refined_translation,
            "time_stage1_sec": end_s1 - start_s1,
            "time_stage2_sec": end_s2 - start_s2,
        }


def load_config(config_path: Optional[str] = None) -> Dict:
    if config_path is None:
        config_path = CONFIG_DIR / "settings.json"
    else:
        config_path = Path(config_path)

    if not config_path.exists():
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y config táº¡i: {config_path}")

    with open(config_path, "r", encoding="utf-8") as file:
        return json.load(file)


def create_pipeline_from_config(config_path: Optional[str] = None) -> HybridTranslationPipelineOllama:
    config = load_config(config_path)

    repo_root = Path(__file__).resolve().parents[2]
    ct2_dir = repo_root / config["stage1_model_dir"]

    return HybridTranslationPipelineOllama(
        ct2_model_dir=str(ct2_dir),
        hf_model_name=config["stage1_hf_name"],
        ollama_model=config.get("ollama_model", "llama3:8b"),
        ollama_host=config.get("ollama_host", "http://localhost:11434"),
        temperature=config.get("temperature", 0.2),
        beam_size=config.get("beam_size", 2),
        use_ollama_only=config.get("use_ollama_only", False),
    )


if __name__ == "__main__":
    print("=" * 70)
    print("DEMO: PIPELINE Dá»ŠCH THUáº¬T LAI (OLLAMA)")
    print("=" * 70)

    try:
        pipeline = create_pipeline_from_config()
    except Exception as exc:  # pragma: no cover - manual demo
        print(f"\nâŒ Lá»—i khá»Ÿi táº¡o pipeline:\n{exc}")
        print("\nğŸ’¡ HÃ£y cháº¡y: python scripts/setup_models.py")
        print("ğŸ’¡ VÃ  Ä‘áº£m báº£o Ollama Ä‘ang cháº¡y")
        sys.exit(1)

    test_texts = [
        "Hello world! This is a test of the translation system.",
        "The enterprise solution must be robust and scalable.",
        "Artificial intelligence is transforming the way we work.",
    ]

    for idx, text in enumerate(test_texts, 1):
        print(f"\n{'=' * 70}")
        print(f"VÄ‚N Báº¢N THá»¬ NGHIá»†M #{idx}")
        print(f"{'=' * 70}")

        result = pipeline.translate_and_refine(text)

        if "error" in result:
            print(f"âŒ Lá»–I: {result['error']}")
            continue

        print("\nğŸ“ Nguá»“n:")
        print(f"   {result['source']}")
        print(f"\nğŸ”„ ThÃ´ (Giai Ä‘oáº¡n 1 - {result['time_stage1_sec']:.3f}s):")
        print(f"   {result['raw_translation']}")
        print(f"\nâœ¨ Tinh chá»‰nh (Giai Ä‘oáº¡n 2 - {result['time_stage2_sec']:.3f}s):")
        print(f"   {result['refined_translation']}")
        print(
            f"\nâ±ï¸  Tá»•ng thá»i gian: {result['time_stage1_sec'] + result['time_stage2_sec']:.3f}s"
        )

    print(f"\n{'=' * 70}")
    print("âœ… DEMO HOÃ€N Táº¤T")
    print("=" * 70)
