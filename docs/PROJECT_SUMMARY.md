# ğŸ“Š PROJECT SUMMARY - TÃ³m táº¯t Dá»± Ã¡n

## ğŸ¯ Má»¥c tiÃªu

Triá»ƒn khai pipeline dá»‹ch thuáº­t lai **Translate-and-Refine** hai giai Ä‘oáº¡n:
1. **Stage 1**: Dá»‹ch thÃ´ nhanh báº±ng mÃ´ hÃ¬nh NMT nháº¹
2. **Stage 2**: Tinh chá»‰nh báº±ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM)

Má»¥c tiÃªu: Káº¿t há»£p **tá»‘c Ä‘á»™** (Stage 1) vÃ  **cháº¥t lÆ°á»£ng** (Stage 2).

---

## ğŸ—ï¸ Kiáº¿n trÃºc

### Stage 1: Neural Machine Translation (NMT)

- **MÃ´ hÃ¬nh**: `Helsinki-NLP/opus-mt-en-vi`
- **Framework**: CTranslate2 (optimized inference engine)
- **LÆ°á»£ng tá»­ hÃ³a**: INT8 (giáº£m VRAM, tÄƒng tá»‘c Ä‘á»™)
- **VRAM**: ~813MB
- **Tá»‘c Ä‘á»™**: ~9x nhanh hÆ¡n PyTorch baseline
- **Vai trÃ²**: Dá»‹ch thÃ´ nhanh tá»« English â†’ Vietnamese

### Stage 2: Large Language Model (LLM)

#### PhiÃªn báº£n Ollama (Khuyáº¿n nghá»‹)

- **MÃ´ hÃ¬nh**: `llama3.2:3b` (hoáº·c `llama3:8b`, `mistral:7b`)
- **Framework**: Ollama API (HTTP REST)
- **Quáº£n lÃ½**: Ollama tá»± Ä‘á»™ng xá»­ lÃ½ VRAM vÃ  model loading
- **Prompt**: English, system message, stop tokens
- **Timeout**: 180s (cÃ³ thá»ƒ Ä‘iá»u chá»‰nh)
- **Vai trÃ²**: Tinh chá»‰nh dá»‹ch thÃ´, cáº£i thiá»‡n fluency vÃ  naturalness

#### PhiÃªn báº£n llama-cpp-python (KhÃ´ng dÃ¹ng)

- **LÃ½ do bá»**: Requires Visual Studio Build Tools trÃªn Windows
- **Thay tháº¿ báº±ng**: Ollama

---

## ğŸ“ Cáº¥u trÃºc Dá»± Ã¡n

### Files chÃ­nh (PhiÃªn báº£n Ollama)

```
VTranslator/
â”‚
â”œâ”€â”€ app_ollama.py              # Gradio UI cho Ollama
â”œâ”€â”€ pipeline_ollama.py         # Core pipeline vá»›i Ollama
â”œâ”€â”€ config_ollama.json         # Cáº¥u hÃ¬nh Ollama
â”œâ”€â”€ setup_models.py            # Script thiáº¿t láº­p mÃ´ hÃ¬nh Stage 1
â”œâ”€â”€ setup_ollama.bat           # Auto-install cho Windows
â”‚
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”‚
â”œâ”€â”€ OLLAMA_GUIDE.md           # HÆ°á»›ng dáº«n chi tiáº¿t Ollama
â”œâ”€â”€ QUICKSTART.md             # Báº¯t Ä‘áº§u nhanh trong 3 bÆ°á»›c
â”œâ”€â”€ README.md                 # Tá»•ng quan dá»± Ã¡n
â”œâ”€â”€ PROJECT_SUMMARY.md        # File nÃ y
â”œâ”€â”€ LICENSE                   # MIT License
â”‚
â””â”€â”€ models/                   # ThÆ° má»¥c lÆ°u mÃ´ hÃ¬nh (tá»± táº¡o)
    â””â”€â”€ opus-mt-en-vi-ct2/    # CTranslate2 model (sau khi cháº¡y setup)
```

### Files legacy (KhÃ´ng dÃ¹ng)

```
â”œâ”€â”€ app.py                    # Gradio UI cho llama-cpp-python
â”œâ”€â”€ pipeline.py               # Core pipeline vá»›i llama-cpp-python
â”œâ”€â”€ config.json               # Cáº¥u hÃ¬nh cho llama-cpp-python
```

---

## ğŸ”§ Cáº¥u hÃ¬nh

### `config_ollama.json`

```json
{
  "stage1_hf_name": "Helsinki-NLP/opus-mt-en-vi",
  "stage1_model_dir": "models/opus-mt-en-vi-ct2",
  "ollama_model": "llama3.2:3b",
  "ollama_host": "http://localhost:11434",
  "temperature": 0.2,
  "timeout": 180
}
```

**Tham sá»‘ quan trá»ng:**
- `ollama_model`: TÃªn mÃ´ hÃ¬nh LLM (xem `ollama list`)
- `temperature`: Äá»™ sÃ¡ng táº¡o (0.1-0.3 cho dá»‹ch thuáº­t)
- `timeout`: Thá»i gian chá» tá»‘i Ä‘a (giÃ¢y)

---

## âš™ï¸ Quy trÃ¬nh Dá»‹ch thuáº­t

### Pipeline Workflow

```
English Input
     â†“
[Tokenization] (sentencepiece)
     â†“
[Stage 1: CTranslate2 NMT]
     â†“
Vietnamese Draft (thÃ´)
     â†“
[Stage 2: Ollama LLM]
     â†“
Vietnamese Refined (tinh chá»‰nh)
```

### Stage 2 Prompt Engineering

**System Message:**
```
You are a translation refinement assistant. Your task is to improve Vietnamese translations.
```

**User Prompt:**
```
Improve this Vietnamese translation:
{translation}

Output only the improved Vietnamese text, nothing else.
```

**Stop Tokens:** `["\n\n", "English:", "Current", "Improved"]`

---

## ğŸ“Š Hiá»‡u nÄƒng

### Tá»‘c Ä‘á»™ (Stage 1 - CTranslate2)

| Metric | GiÃ¡ trá»‹ |
|--------|---------|
| Inference Speed | ~9x faster than PyTorch |
| VRAM Usage | ~813MB (INT8 quantization) |
| Model Size | ~300MB (on disk) |

### Cháº¥t lÆ°á»£ng (Stage 2 - Ollama)

| Metric | MÃ´ táº£ |
|--------|-------|
| Fluency | Cáº£i thiá»‡n tÃ­nh tá»± nhiÃªn cá»§a cÃ¢u |
| Consistency | Giá»¯ nguyÃªn Ã½ nghÄ©a gá»‘c |
| Naturalness | Tiáº¿ng Viá»‡t tá»± nhiÃªn hÆ¡n, Ã­t mÃ¡y mÃ³c |

---

## ğŸš€ CÃ i Ä‘áº·t & Sá»­ dá»¥ng

### Quick Start

```bash
# 1. Tá»± Ä‘á»™ng (Windows)
setup_ollama.bat

# 2. Thá»§ cÃ´ng
pip install ctranslate2 transformers sentencepiece gradio requests torch
python setup_models.py
ollama pull llama3.2:3b

# 3. Cháº¡y
ollama serve
python app_ollama.py
```

Xem **QUICKSTART.md** Ä‘á»ƒ biáº¿t chi tiáº¿t.

---

## ğŸ›ï¸ TÃ¹y chá»‰nh

### Thay Ä‘á»•i mÃ´ hÃ¬nh LLM

1. Chá»‰nh sá»­a `config_ollama.json`:
   ```json
   "ollama_model": "llama3:8b"
   ```

2. Táº£i mÃ´ hÃ¬nh:
   ```bash
   ollama pull llama3:8b
   ```

3. Restart `app_ollama.py`

### Äiá»u chá»‰nh prompt

Chá»‰nh sá»­a `pipeline_ollama.py`:

```python
def _refine_stage2(self, translation: str) -> str:
    system = "Your custom system message"
    user = f"Your custom user prompt with {translation}"
    # ...
```

### KhÃ´ng giá»›i háº¡n Ä‘á»™ dÃ i

Pipeline **KHÃ”NG cÃ³ giá»›i háº¡n** vá» Ä‘á»™ dÃ i vÄƒn báº£n:
- âœ… `app_ollama.py`: KhÃ´ng cÃ³ check Ä‘á»™ dÃ i
- âœ… `pipeline_ollama.py`: `num_predict` tá»± Ä‘á»™ng tá»« 512-4096 tokens
- âœ… `timeout`: 180s (cÃ³ thá»ƒ tÄƒng lÃªn)

---

## ğŸ†š So sÃ¡nh PhiÃªn báº£n

| Feature | llama-cpp-python | Ollama |
|---------|------------------|--------|
| **Windows Install** | âŒ Cáº§n Build Tools | âœ… Chá»‰ cáº§n .exe |
| **Model Management** | ğŸ”§ Thá»§ cÃ´ng (GGUF) | âœ… CLI (pull/rm) |
| **API** | âš™ï¸ Python binding | âœ… HTTP REST |
| **Performance** | âš¡ Fast | âš¡ Equivalent |
| **Ease of Use** | ğŸ”´ Advanced users | ğŸŸ¢ Beginner friendly |
| **Recommendation** | Linux/Mac experts | **Everyone** |

---

## ğŸ› Troubleshooting

### Lá»—i: "Could not connect to Ollama"

```bash
ollama serve
```

### Lá»—i: "Model not found"

```bash
ollama list
ollama pull llama3.2:3b
```

### Lá»—i: "Timeout"

TÄƒng `timeout` trong `config_ollama.json`:
```json
"timeout": 300
```

Xem **OLLAMA_GUIDE.md** Ä‘á»ƒ biáº¿t thÃªm.

---

## ğŸ“š Dependencies

### Python Packages

```
ctranslate2>=3.24.0
transformers>=4.36.0
sentencepiece>=0.1.99
torch>=2.1.0
gradio>=4.0.0
requests>=2.31.0
```

### External Tools

- **Ollama**: https://ollama.ai/download
- **Python**: 3.8+ (khuyáº¿n nghá»‹ 3.10+)

---

## ğŸ“– TÃ i liá»‡u

- **QUICKSTART.md** - Báº¯t Ä‘áº§u nhanh trong 3 bÆ°á»›c
- **OLLAMA_GUIDE.md** - HÆ°á»›ng dáº«n chi tiáº¿t vá» Ollama
- **README.md** - Tá»•ng quan vÃ  giá»›i thiá»‡u
- **LICENSE** - MIT License

---

## ğŸ¯ Káº¿t luáº­n

Dá»± Ã¡n VTranslator cung cáº¥p:
- âœ… **Pipeline dá»‹ch thuáº­t lai** hiá»‡u quáº£ (Translate-and-Refine)
- âœ… **KhÃ´ng giá»›i háº¡n Ä‘á»™ dÃ i** vÄƒn báº£n Ä‘áº§u vÃ o
- âœ… **Dá»… cÃ i Ä‘áº·t** trÃªn Windows vá»›i Ollama
- âœ… **Linh hoáº¡t** trong viá»‡c chá»n mÃ´ hÃ¬nh LLM
- âœ… **Hiá»‡u nÄƒng cao** vá»›i CTranslate2 vÃ  Ollama

**PhiÃªn báº£n Ollama** lÃ  lá»±a chá»n khuyáº¿n nghá»‹ cho má»i ngÆ°á»i dÃ¹ng, Ä‘áº·c biá»‡t trÃªn Windows.

---

**ğŸš€ Báº¯t Ä‘áº§u ngay: `setup_ollama.bat`**
