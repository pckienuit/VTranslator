# ğŸ“Š PROJECT SUMMARY - TÃ³m táº¯t Dá»± Ã¡n

## ğŸ¯ Má»¥c tiÃªu

Triá»ƒn khai pipeline dá»‹ch thuáº­t lai **Translate-and-Refine** hai giai Ä‘oáº¡n:
1. **Stage 1**: Dá»‹ch thÃ´ nhanh báº±ng mÃ´ hÃ¬nh NMT nháº¹
2. **Stage 2**: Tinh chá»‰nh báº±ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM)

Má»¥c tiÃªu: Káº¿t há»£p **tá»‘c Ä‘á»™** (Stage 1) vÃ  **cháº¥t lÆ°á»£ng** (Stage 2).

---

## ğŸ—ï¸ Kiáº¿n trÃºc

### Stage 1: Neural Machine Translation (NMT)

- **MÃ´ hÃ¬nh**: `Helsinki-NLP/opus-mt-en-vi`
- **Framework**: CTranslate2 (optimized inference engine)
- **LÆ°á»£ng tá»­ hÃ³a**: INT8 (giáº£m VRAM, tÄƒng tá»‘c Ä‘á»™)
- **VRAM**: ~813MB
- **Tá»‘c Ä‘á»™**: ~9x nhanh hÆ¡n PyTorch baseline
- **Vai trÃ²**: Dá»‹ch thÃ´ nhanh tá»« English â†’ Vietnamese

### Stage 2: Large Language Model (LLM)

#### PhiÃªn báº£n Ollama (Khuyáº¿n nghá»‹)

- **MÃ´ hÃ¬nh**: `llama3.2:3b` (hoáº·c `llama3:8b`, `mistral:7b`)
- **Framework**: Ollama API (HTTP REST)
- **Quáº£n lÃ½**: Ollama tá»± Ä‘á»™ng xá»­ lÃ½ VRAM vÃ  model loading
- **Prompt**: English, system message, stop tokens
- **Timeout**: 180s (cÃ³ thá»ƒ Ä‘iá»u chá»‰nh)
- **Vai trÃ²**: Tinh chá»‰nh dá»‹ch thÃ´, cáº£i thiá»‡n fluency vÃ  naturalness

#### PhiÃªn báº£n llama-cpp-python (KhÃ´ng dÃ¹ng)

- **LÃ½ do bá»**: Requires Visual Studio Build Tools trÃªn Windows
- **Thay tháº¿ báº±ng**: Ollama

---

## ğŸ“ Cáº¥u trÃºc Dá»± Ã¡n

### Files chÃ­nh (PhiÃªn báº£n Ollama)

```
VTranslator/
â”‚
â”œâ”€â”€ app_ollama.py              # Gradio UI cho Ollama
â”œâ”€â”€ pipeline_ollama.py         # Core pipeline vá»›i Ollama
â”œâ”€â”€ config_ollama.json         # Cáº¥u hÃ¬nh Ollama
â”œâ”€â”€ setup_models.py            # Script thiáº¿t láº­p mÃ´ hÃ¬nh Stage 1
â”œâ”€â”€ setup_ollama.bat           # Auto-install cho Windows
â”‚
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”‚
â”œâ”€â”€ OLLAMA_GUIDE.md           # HÆ°á»›ng dáº«n chi tiáº¿t Ollama
â”œâ”€â”€ QUICKSTART.md             # Báº¯t Ä‘áº§u nhanh trong 3 bÆ°á»›c
â”œâ”€â”€ README.md                 # Tá»•ng quan dá»± Ã¡n
â”œâ”€â”€ PROJECT_SUMMARY.md        # File nÃ y
â”œâ”€â”€ LICENSE                   # MIT License
â”‚
â””â”€â”€ models/                   # ThÆ° má»¥c lÆ°u mÃ´ hÃ¬nh (tá»± táº¡o)
    â””â”€â”€ opus-mt-en-vi-ct2/    # CTranslate2 model (sau khi cháº¡y setup)
```

### Files legacy (KhÃ´ng dÃ¹ng)

```
â”œâ”€â”€ app.py                    # Gradio UI cho llama-cpp-python
â”œâ”€â”€ pipeline.py               # Core pipeline vá»›i llama-cpp-python
â”œâ”€â”€ config.json               # Cáº¥u hÃ¬nh cho llama-cpp-python
```

---

## ğŸ”§ Cáº¥u hÃ¬nh

### `config_ollama.json`

```json
{
  "stage1_hf_name": "Helsinki-NLP/opus-mt-en-vi",
  "stage1_model_dir": "models/opus-mt-en-vi-ct2",
  "ollama_model": "llama3.2:3b",
  "ollama_host": "http://localhost:11434",
  "temperature": 0.2,
  "timeout": 180
}
```

**Tham sá»‘ quan trá»ng:**
- `ollama_model`: TÃªn mÃ´ hÃ¬nh LLM (xem `ollama list`)
# ğŸ“Š PROJECT SUMMARY

## ğŸ¯ Má»¥c tiÃªu
Cung cáº¥p trÃ¬nh dá»‹ch Anh â†’ Viá»‡t cháº¥t lÆ°á»£ng cao dá»±a trÃªn **má»™t giai Ä‘oáº¡n duy nháº¥t** cá»§a mÃ´ hÃ¬nh `gemma3:12b` cháº¡y qua Ollama. Pipeline táº­p trung vÃ o sá»± Ä‘Æ¡n giáº£n: chá»‰ cáº§n cÃ i Ollama, kÃ©o model, cáº¥u hÃ¬nh thÃ´ng sá»‘, rá»“i cháº¡y UI Gradio.

---

## ğŸ—ï¸ Kiáº¿n trÃºc hiá»‡n táº¡i
```
VÄƒn báº£n nguá»“n + bá»‘i cáº£nh
        â†“
[GemmaTranslationPipeline]
        â†“
Káº¿t quáº£ dá»‹ch Ä‘Ã£ tinh chá»‰nh
```
- KhÃ´ng cÃ²n Stage 1 NMT; Gemma Ä‘áº£m nhiá»‡m cáº£ dá»‹ch thÃ´ láº«n tinh chá»‰nh.
- Pipeline chia vÄƒn báº£n thÃ nh tá»«ng chunk, thÃªm prompt hÆ°á»›ng dáº«n giá»¯ nguyÃªn thuáº­t ngá»¯, rá»“i ghÃ©p káº¿t quáº£ cuá»‘i.
- UI (`src/app/web_ui.py`) giao tiáº¿p vá»›i pipeline thÃ´ng qua API cá»¥c bá»™.

---

## ğŸ“ Cáº¥u trÃºc chÃ­nh
```
VTranslator/
â”œâ”€â”€ run_app.py                    # Entry point Gradio
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/web_ui.py            # Giao diá»‡n web
â”‚   â”œâ”€â”€ pipeline/gemma_pipeline.py
â”‚   â”œâ”€â”€ pipeline/hybrid_pipeline.py (shim tÆ°Æ¡ng thÃ­ch)
â”‚   â””â”€â”€ config/settings.json     # Tham sá»‘ Ollama + chunk
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_models.py          # Kiá»ƒm tra phá»¥ thuá»™c, hÆ°á»›ng dáº«n kÃ©o model
â”‚   â””â”€â”€ setup_ollama.bat         # Quy trÃ¬nh cÃ i Ä‘áº·t trÃªn Windows
â”œâ”€â”€ docs/                        # README, QUICKSTART, INSTALL_WINDOWS, OLLAMA_GUIDE, ...
â”œâ”€â”€ requirements.txt             # Chá»‰ cáº§n gradio + requests
â””â”€â”€ README.md                    # Tá»•ng quan dá»± Ã¡n
```

---

## âš™ï¸ Cáº¥u hÃ¬nh
`src/config/settings.json` Ä‘iá»u khiá»ƒn toÃ n bá»™ hÃ nh vi:
```json
{
  "ollama_model": "gemma3:12b",
  "ollama_host": "http://localhost:11434",
  "temperature": 0.15,
  "max_tokens": 2048,
  "max_chunk_chars": 2100,
  "timeout": 120.0
}
```
- `max_chunk_chars`: giá»›i háº¡n sá»‘ kÃ½ tá»± má»™t Ä‘oáº¡n gá»­i lÃªn Ollama; tá»± Ä‘á»™ng ghÃ©p láº¡i.
- `max_tokens`: sá»‘ token tráº£ vá» tá»‘i Ä‘a má»—i láº§n gá»i.
- `timeout`: thá»i gian chá» API.

---

## ğŸ§  Prompt Engineering
System prompt nháº¯c Gemma Ä‘Ã³ng vai dá»‹ch giáº£ ká»¹ thuáº­t, giá»¯ Ä‘á»‹nh dáº¡ng, khÃ´ng bá» ná»™i dung. User prompt bao gá»“m:
1. Ngá»¯ cáº£nh/bá»‘i cáº£nh dá»‹ch
2. VÄƒn báº£n gá»‘c tiáº¿ng Anh
3. HÆ°á»›ng dáº«n yÃªu cáº§u Ä‘áº§u ra tiáº¿ng Viá»‡t rÃµ rÃ ng

Pipeline tá»± Ä‘á»™ng loáº¡i bá» nhÃ£n â€œVietnamese Translation:â€ náº¿u model tráº£ vá».

---

## ğŸš€ Quy trÃ¬nh sá»­ dá»¥ng
1. CÃ i Ollama (xem `docs/INSTALL_WINDOWS.md` hoáº·c `docs/OLLAMA_GUIDE.md`).
2. Cháº¡y `ollama pull gemma3:12b`.
3. Táº¡o mÃ´i trÆ°á»ng Python vÃ  `pip install -r requirements.txt`.
4. Cháº¡y `python run_app.py`.
5. Má»Ÿ URL Gradio, nháº­p vÄƒn báº£n + bá»‘i cáº£nh, nháº¥n "Translate".

---

## ğŸ“ˆ Hiá»‡u nÄƒng & Giá»›i háº¡n
- Cháº¥t lÆ°á»£ng dá»‹ch phá»¥ thuá»™c vÃ o Gemma; tá»‘c Ä‘á»™ ~5-10s cho Ä‘oáº¡n 400 tá»« (tÃ¹y pháº§n cá»©ng).
- KhÃ´ng giá»›i háº¡n Ä‘á»™ dÃ i vÄƒn báº£n; pipeline sáº½ tá»± chia nhá».
- Cáº§n RAM tá»‘i thiá»ƒu 8GB vÃ  khoáº£ng 15GB Ä‘Ä©a trá»‘ng cho model.

---

## ğŸ”§ TÃ¹y chá»‰nh
- Muá»‘n dÃ¹ng model khÃ¡c: sá»­a `ollama_model` vÃ  kÃ©o model tÆ°Æ¡ng á»©ng.
- Muá»‘n thay prompt: chá»‰nh `GemmaTranslationPipeline._build_prompt`.
- Muá»‘n Ä‘á»•i UI: cáº­p nháº­t `src/app/web_ui.py` (Gradio components).

---

## ğŸ“š TÃ i liá»‡u liÃªn quan
- `README.md` â€“ Tá»•ng quan vÃ  hÆ°á»›ng dáº«n chung
- `docs/QUICKSTART.md` â€“ 3 bÆ°á»›c cháº¡y nhanh
- `docs/INSTALL_WINDOWS.md` â€“ HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ cho Windows
- `docs/OLLAMA_GUIDE.md` â€“ Chi tiáº¿t thiáº¿t láº­p Ollama
- `docs/MODELS.md` â€“ ThÃ´ng tin model Gemma Ä‘ang dÃ¹ng

---

## âœ… Káº¿t luáº­n
PhiÃªn báº£n hiá»‡n táº¡i nháº¥n máº¡nh sá»± Ä‘Æ¡n giáº£n: má»™t mÃ´ hÃ¬nh duy nháº¥t, cáº¥u hÃ¬nh nháº¹, dá»… váº­n hÃ nh. Chá»‰ cáº§n Ollama + Gemma 3 12B lÃ  cÃ³ thá»ƒ dá»‹ch cháº¥t lÆ°á»£ng cao mÃ  khÃ´ng pháº£i quáº£n lÃ½ nhiá»u pipeline phá»©c táº¡p.
Chá»‰nh sá»­a `pipeline_ollama.py`:
