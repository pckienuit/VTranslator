# ğŸ¦™ HÆ°á»›ng dáº«n Sá»­ dá»¥ng Pipeline Dá»‹ch thuáº­t vá»›i Ollama

## ğŸ“‹ Giá»›i thiá»‡u

PhiÃªn báº£n Ollama cá»§a pipeline dá»‹ch thuáº­t lai sá»­ dá»¥ng **Ollama** Ä‘á»ƒ quáº£n lÃ½ mÃ´ hÃ¬nh LLM Stage 2, thay vÃ¬ táº£i file GGUF thá»§ cÃ´ng. CÃ¡ch nÃ y Ä‘Æ¡n giáº£n hÆ¡n nhiá»u, Ä‘áº·c biá»‡t trÃªn Windows.

### Lá»£i Ã­ch cá»§a Ollama

- âœ… **KhÃ´ng cáº§n biÃªn dá»‹ch C++**: KhÃ´ng cáº§n Visual Studio Build Tools
- âœ… **Quáº£n lÃ½ mÃ´ hÃ¬nh dá»… dÃ ng**: Táº£i, cáº­p nháº­t, xÃ³a mÃ´ hÃ¬nh báº±ng 1 lá»‡nh
- âœ… **API Ä‘Æ¡n giáº£n**: Giao tiáº¿p qua HTTP REST API
- âœ… **Tá»± Ä‘á»™ng tá»‘i Æ°u**: Ollama tá»± Ä‘á»™ng quáº£n lÃ½ VRAM vÃ  hiá»‡u nÄƒng
- âœ… **Äa ná»n táº£ng**: Há»— trá»£ Windows, macOS, Linux

---

## ğŸš€ CÃ i Ä‘áº·t Nhanh

### 1. CÃ i Ä‘áº·t Ollama

#### Windows
```bash
# Táº£i tá»« trang chá»§
https://ollama.ai/download

# Cháº¡y file .exe vÃ  cÃ i Ä‘áº·t
# Ollama sáº½ tá»± Ä‘á»™ng thÃªm vÃ o PATH
```

#### macOS
```bash
brew install ollama
```

#### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Kiá»ƒm tra cÃ i Ä‘áº·t

```bash
ollama --version
```

Náº¿u tháº¥y sá»‘ phiÃªn báº£n (VD: `0.1.23`) â†’ ThÃ nh cÃ´ng!

---

## ğŸ› ï¸ Thiáº¿t láº­p Pipeline

### 1. CÃ i Ä‘áº·t thÆ° viá»‡n Python

```bash
pip install ctranslate2 transformers sentencepiece gradio requests torch
```

### 2. Thiáº¿t láº­p mÃ´ hÃ¬nh Stage 1 (CTranslate2)

```bash
python scripts/setup_models.py
```

Script nÃ y sáº½:
- Táº£i mÃ´ hÃ¬nh `Helsinki-NLP/opus-mt-en-vi`
- Chuyá»ƒn Ä‘á»•i sang Ä‘á»‹nh dáº¡ng CTranslate2 vá»›i lÆ°á»£ng tá»­ hÃ³a INT8

### 3. Táº£i mÃ´ hÃ¬nh LLM cho Ollama (Stage 2)

```bash
ollama pull llama3.2:3b
```

MÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c táº£i vá» (~2GB) vÃ  lÆ°u trong cache cá»§a Ollama.

**CÃ¡c mÃ´ hÃ¬nh kháº£ dá»¥ng:**
- `llama3.2:3b` - Nháº¹, nhanh (khuyáº¿n nghá»‹ cho mÃ¡y 8GB RAM)
- `llama3:8b` - CÃ¢n báº±ng, cháº¥t lÆ°á»£ng tá»‘t
- `gemma:7b` - Thay tháº¿ Google Gemma
- `mistral:7b` - Cháº¥t lÆ°á»£ng cao

Xem danh sÃ¡ch Ä‘áº§y Ä‘á»§: https://ollama.ai/library

---

## â–¶ï¸ Cháº¡y Pipeline

### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Ollama Server

```bash
ollama serve
```

Ollama sáº½ cháº¡y á»Ÿ `http://localhost:11434`

**LÆ°u Ã½:** 
- TrÃªn Windows/macOS, Ollama thÆ°á»ng tá»± Ä‘á»™ng cháº¡y ná»n sau khi cÃ i Ä‘áº·t
- Kiá»ƒm tra báº±ng cÃ¡ch má»Ÿ: http://localhost:11434 (sáº½ tháº¥y "Ollama is running")

### BÆ°á»›c 2: Cháº¡y á»©ng dá»¥ng Gradio

Má»Ÿ terminal/command prompt **má»›i** vÃ  cháº¡y:

```bash
python run_app.py
```

á»¨ng dá»¥ng sáº½ má»Ÿ táº¡i: http://localhost:7860

---

## ğŸ“ Sá»­ dá»¥ng Pipeline

### Giao diá»‡n Gradio

1. Má»Ÿ trÃ¬nh duyá»‡t: http://localhost:7860
2. DÃ¡n vÄƒn báº£n tiáº¿ng Anh vÃ o Ã´ **Input**
3. Nháº¥n **Translate**
4. Káº¿t quáº£ hiá»ƒn thá»‹ á»Ÿ **Stage 1 Output** (dá»‹ch thÃ´) vÃ  **Stage 2 Output** (dá»‹ch tinh chá»‰nh)

### VÃ­ dá»¥

**Input:**
```
Artificial intelligence is transforming how we interact with technology.
```

**Stage 1 Output (Dá»‹ch thÃ´):**
```
TrÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘ang chuyá»ƒn Ä‘á»•i cÃ¡ch chÃºng ta tÆ°Æ¡ng tÃ¡c vá»›i cÃ´ng nghá»‡.
```

**Stage 2 Output (Tinh chá»‰nh):**
```
TrÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘ang thay Ä‘á»•i cÃ¡ch chÃºng ta tÆ°Æ¡ng tÃ¡c vá»›i cÃ´ng nghá»‡.
```

### KhÃ´ng giá»›i háº¡n Ä‘á»™ dÃ i

Pipeline **KHÃ”NG cÃ³ giá»›i háº¡n** vá» Ä‘á»™ dÃ i vÄƒn báº£n Ä‘áº§u vÃ o. Báº¡n cÃ³ thá»ƒ dá»‹ch:
- Äoáº¡n vÄƒn ngáº¯n (vÃ i cÃ¢u)
- BÃ i viáº¿t dÃ i (hÃ ng ngÃ n tá»«)
- TÃ i liá»‡u ká»¹ thuáº­t
- SÃ¡ch, bÃ¡o cÃ¡o

**LÆ°u Ã½ hiá»‡u nÄƒng:**
- VÄƒn báº£n dÃ i hÆ¡n â†’ Thá»i gian xá»­ lÃ½ lÃ¢u hÆ¡n
- Ollama tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh `num_predict` theo Ä‘á»™ dÃ i (512-4096 tokens)
- Timeout máº·c Ä‘á»‹nh: 180 giÃ¢y (cÃ³ thá»ƒ tÄƒng trong `src/config/settings.json`)

---

## âš™ï¸ Cáº¥u hÃ¬nh NÃ¢ng cao

### File `src/config/settings.json`

```json
{
   "stage1_hf_name": "Helsinki-NLP/opus-mt-en-vi",
   "stage1_model_dir": "models/opus-mt-en-vi-ct2",
   "ollama_model": "llama3.2:3b",
   "ollama_host": "http://localhost:11434",
   "temperature": 0.2,
   "timeout": 180
}
```

### Thay Ä‘á»•i mÃ´ hÃ¬nh LLM

1. Chá»‰nh sá»­a `ollama_model` trong `src/config/settings.json`:
   ```json
   "ollama_model": "llama3:8b"
   ```

2. Táº£i mÃ´ hÃ¬nh má»›i:
   ```bash
   ollama pull llama3:8b
   ```

3. Khá»Ÿi Ä‘á»™ng láº¡i `python run_app.py`

### Äiá»u chá»‰nh tham sá»‘

- **`temperature`** (0.0 - 1.0): Äá»™ sÃ¡ng táº¡o
  - `0.1-0.3`: Dá»‹ch nháº¥t quÃ¡n, Ã­t biáº¿n thá»ƒ (khuyáº¿n nghá»‹)
  - `0.5-0.7`: CÃ¢n báº±ng
  - `0.8-1.0`: Dá»‹ch linh hoáº¡t, nhiá»u biáº¿n thá»ƒ

- **`timeout`** (giÃ¢y): Thá»i gian chá» tá»‘i Ä‘a
  - TÄƒng lÃªn náº¿u vÄƒn báº£n ráº¥t dÃ i: `"timeout": 300`

---

## ğŸ”§ Quáº£n lÃ½ MÃ´ hÃ¬nh Ollama

### Xem danh sÃ¡ch mÃ´ hÃ¬nh Ä‘Ã£ táº£i

```bash
ollama list
```

### Táº£i mÃ´ hÃ¬nh má»›i

```bash
ollama pull <model_name>
# VÃ­ dá»¥:
ollama pull mistral:7b
```

### XÃ³a mÃ´ hÃ¬nh khÃ´ng dÃ¹ng

```bash
ollama rm <model_name>
# VÃ­ dá»¥:
ollama rm llama3.2:3b
```

### Cáº­p nháº­t mÃ´ hÃ¬nh

```bash
ollama pull <model_name>
# Ollama sáº½ tá»± Ä‘á»™ng kiá»ƒm tra vÃ  táº£i phiÃªn báº£n má»›i nháº¥t
```

---

## ğŸ› Xá»­ lÃ½ Lá»—i

### Lá»—i: "Could not connect to Ollama"

**NguyÃªn nhÃ¢n:** Ollama server chÆ°a cháº¡y

**Giáº£i phÃ¡p:**
```bash
ollama serve
```

Hoáº·c kiá»ƒm tra Ollama Ä‘Ã£ cháº¡y ná»n chÆ°a:
- Windows: Kiá»ƒm tra System Tray
- macOS: Kiá»ƒm tra Menu Bar
- Linux: `ps aux | grep ollama`

### Lá»—i: "Model not found"

**NguyÃªn nhÃ¢n:** MÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c táº£i

**Giáº£i phÃ¡p:**
```bash
ollama pull llama3.2:3b
```

### Lá»—i: "Timeout waiting for response"

**NguyÃªn nhÃ¢n:** VÄƒn báº£n quÃ¡ dÃ i hoáº·c mÃ´ hÃ¬nh cháº­m

**Giáº£i phÃ¡p:**
1. TÄƒng `timeout` trong `src/config/settings.json`:
   ```json
   "timeout": 300
   ```

2. Hoáº·c dÃ¹ng mÃ´ hÃ¬nh nhá» hÆ¡n:
   ```bash
   ollama pull llama3.2:3b
   ```

### Lá»—i: CTranslate2 khÃ´ng tÃ¬m tháº¥y mÃ´ hÃ¬nh

**NguyÃªn nhÃ¢n:** ChÆ°a cháº¡y `python scripts/setup_models.py`

**Giáº£i phÃ¡p:**
```bash
python scripts/setup_models.py
```

---

## ğŸ“Š So sÃ¡nh PhiÃªn báº£n

| TÃ­nh nÄƒng | llama-cpp-python | Ollama |
|-----------|------------------|--------|
| CÃ i Ä‘áº·t Windows | âŒ Cáº§n Build Tools | âœ… Chá»‰ cáº§n .exe |
| Quáº£n lÃ½ mÃ´ hÃ¬nh | ğŸ”§ Thá»§ cÃ´ng (GGUF) | âœ… Tá»± Ä‘á»™ng (CLI) |
| API | âš™ï¸ Python binding | âœ… HTTP REST |
| Hiá»‡u nÄƒng | âš¡ Nhanh | âš¡ TÆ°Æ¡ng Ä‘Æ°Æ¡ng |
| Khuyáº¿n nghá»‹ | Cho ngÆ°á»i dÃ¹ng Linux/Mac cÃ³ kinh nghiá»‡m | **Cho má»i ngÆ°á»i, Ä‘áº·c biá»‡t Windows** |

---

## ğŸ¯ Tips & Tricks

### 1. TÄƒng tá»‘c Ä‘á»™ dá»‹ch

- DÃ¹ng mÃ´ hÃ¬nh nhá» hÆ¡n (`llama3.2:3b` thay vÃ¬ `llama3:8b`)
- Giáº£m `temperature` xuá»‘ng `0.1-0.2`
- Äáº£m báº£o Ollama cháº¡y trÃªn GPU (tá»± Ä‘á»™ng náº¿u cÃ³ CUDA/Metal)

### 2. TÄƒng cháº¥t lÆ°á»£ng dá»‹ch

- DÃ¹ng mÃ´ hÃ¬nh lá»›n hÆ¡n (`llama3:8b`, `mistral:7b`)
- TÄƒng `temperature` lÃªn `0.3-0.4`
- Chá»‰nh sá»­a prompt trong `src/pipeline/hybrid_pipeline.py` (náº¿u cáº§n)

### 3. Dá»‹ch hÃ ng loáº¡t

Táº¡o script Python:

```python
from src.pipeline import HybridTranslationPipelineOllama

pipeline = HybridTranslationPipelineOllama.from_config()

texts = [
    "First text to translate.",
    "Second text to translate.",
    # ... many more
]

for i, text in enumerate(texts):
    stage1, stage2 = pipeline.translate(text)
    print(f"[{i+1}] Stage 2: {stage2}\n")
```

### 4. Cháº¡y Ollama tá»« xa

Náº¿u Ollama cháº¡y trÃªn mÃ¡y khÃ¡c, Ä‘á»•i `ollama_host`:

```json
"ollama_host": "http://192.168.1.100:11434"
```

---

## ğŸ†˜ Há»— trá»£

### TÃ i liá»‡u

- Ollama: https://ollama.ai/
- CTranslate2: https://github.com/OpenNMT/CTranslate2
- Gradio: https://gradio.app/

### BÃ¡o lá»—i

Náº¿u gáº·p lá»—i, hÃ£y kiá»ƒm tra:
1. Ollama Ä‘Ã£ cháº¡y chÆ°a: `curl http://localhost:11434`
2. MÃ´ hÃ¬nh Ä‘Ã£ táº£i chÆ°a: `ollama list`
3. ThÆ° viá»‡n Python: `pip list | grep ctranslate2`

---

## ğŸš€ BÆ°á»›c tiáº¿p theo

Sau khi thiáº¿t láº­p thÃ nh cÃ´ng, báº¡n cÃ³ thá»ƒ:

1. **TÃ¹y chá»‰nh giao diá»‡n**: Chá»‰nh sá»­a `src/app/web_ui.py` (Gradio)
2. **Thá»­ mÃ´ hÃ¬nh khÃ¡c**: `ollama pull mistral:7b`
3. **TÃ­ch há»£p vÃ o á»©ng dá»¥ng**: Import `HybridTranslationPipeline`
4. **Triá»ƒn khai lÃªn server**: DÃ¹ng Gradio sharing hoáº·c Docker

---

**ğŸ‰ ChÃºc báº¡n dá»‹ch thuáº­t hiá»‡u quáº£ vá»›i Ollama!**
