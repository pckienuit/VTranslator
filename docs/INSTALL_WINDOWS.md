# ğŸªŸ HÆ°á»›ng dáº«n CÃ i Ä‘áº·t trÃªn Windows

HÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c Ä‘á»ƒ cÃ i Ä‘áº·t pipeline dá»‹ch thuáº­t lai trÃªn Windows.

---

## âš™ï¸ YÃªu cáº§u Há»‡ thá»‘ng

- **Há»‡ Ä‘iá»u hÃ nh**: Windows 10/11 (64-bit)
- **RAM**: Tá»‘i thiá»ƒu 8GB (khuyáº¿n nghá»‹ 16GB)
- **á»” cá»©ng**: ~5GB trá»‘ng
- **GPU**: KhÃ´ng báº¯t buá»™c (nhÆ°ng tÄƒng tá»‘c náº¿u cÃ³ NVIDIA GPU vá»›i CUDA)

---

## ğŸ“‹ CÃ i Ä‘áº·t ThÃ nh pháº§n

### 1. CÃ i Ä‘áº·t Python

#### BÆ°á»›c 1: Táº£i Python

Truy cáº­p: https://www.python.org/downloads/

Táº£i phiÃªn báº£n **Python 3.10** hoáº·c **3.11** (khuyáº¿n nghá»‹)

#### BÆ°á»›c 2: Cháº¡y trÃ¬nh cÃ i Ä‘áº·t

**Quan trá»ng**: âœ… **TÃ­ch chá»n "Add Python to PATH"**

![Python Install](https://docs.python.org/3/_images/win_installer.png)

Chá»n "Install Now"

#### BÆ°á»›c 3: Kiá»ƒm tra cÃ i Ä‘áº·t

Má»Ÿ **Command Prompt** (Win + R â†’ `cmd` â†’ Enter)

```bash
python --version
```

Káº¿t quáº£: `Python 3.10.x` hoáº·c `Python 3.11.x` â†’ ThÃ nh cÃ´ng!

---

### 2. CÃ i Ä‘áº·t Ollama

#### BÆ°á»›c 1: Táº£i Ollama

Truy cáº­p: https://ollama.ai/download

Chá»n **Windows** â†’ Táº£i file `.exe`

#### BÆ°á»›c 2: Cháº¡y trÃ¬nh cÃ i Ä‘áº·t

Double-click file `OllamaSetup.exe` vÃ  lÃ m theo hÆ°á»›ng dáº«n.

#### BÆ°á»›c 3: Kiá»ƒm tra cÃ i Ä‘áº·t

Má»Ÿ **Command Prompt** má»›i:

```bash
ollama --version
```

Káº¿t quáº£: `ollama version 0.x.x` â†’ ThÃ nh cÃ´ng!

---

## ğŸš€ Thiáº¿t láº­p Pipeline

### Tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

Double-click file `scripts\\setup_ollama.bat` hoáº·c cháº¡y:

```bash
scripts\\setup_ollama.bat
```

Script sáº½ tá»± Ä‘á»™ng:
1. âœ… Kiá»ƒm tra Python
2. âœ… CÃ i Ä‘áº·t thÆ° viá»‡n Python
3. âœ… Táº£i vÃ  chuyá»ƒn Ä‘á»•i mÃ´ hÃ¬nh Stage 1
4. âœ… HÆ°á»›ng dáº«n táº£i mÃ´ hÃ¬nh LLM

LÃ m theo hÆ°á»›ng dáº«n hiá»ƒn thá»‹ sau khi script cháº¡y xong.

---

### Thá»§ cÃ´ng (Náº¿u script tá»± Ä‘á»™ng lá»—i)

#### BÆ°á»›c 1: Clone/Download dá»± Ã¡n

```bash
# Náº¿u cÃ³ Git
git clone <repository-url> VTranslator
cd VTranslator

# Hoáº·c download ZIP vÃ  giáº£i nÃ©n
```

#### BÆ°á»›c 2: Táº¡o mÃ´i trÆ°á»ng áº£o (TÃ¹y chá»n nhÆ°ng khuyáº¿n nghá»‹)

```bash
python -m venv venv
venv\Scripts\activate
```

#### BÆ°á»›c 3: CÃ i Ä‘áº·t thÆ° viá»‡n Python

```bash
pip install -r requirements.txt
```

**LÆ°u Ã½:** Náº¿u gáº·p lá»—i káº¿t ná»‘i, thÃªm `--proxy` náº¿u báº¡n Ä‘ang sau proxy.

#### BÆ°á»›c 4: Thiáº¿t láº­p mÃ´ hÃ¬nh Stage 1

```bash
python scripts/setup_models.py
```

QuÃ¡ trÃ¬nh nÃ y sáº½:
- Táº£i mÃ´ hÃ¬nh `Helsinki-NLP/opus-mt-en-vi` (~300MB)
- Chuyá»ƒn Ä‘á»•i sang CTranslate2 vá»›i lÆ°á»£ng tá»­ hÃ³a INT8

Thá»i gian: 3-5 phÃºt (tÃ¹y tá»‘c Ä‘á»™ máº¡ng)

#### BÆ°á»›c 5: Táº£i mÃ´ hÃ¬nh LLM cho Ollama

```bash
ollama pull llama3.2:3b
```

MÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c táº£i vá» (~2GB). Thá»i gian: 5-10 phÃºt

---

## â–¶ï¸ Cháº¡y á»¨ng dá»¥ng

### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Ollama Server

Ollama thÆ°á»ng tá»± Ä‘á»™ng cháº¡y ná»n sau khi cÃ i Ä‘áº·t trÃªn Windows.

Kiá»ƒm tra báº±ng cÃ¡ch má»Ÿ trÃ¬nh duyá»‡t: http://localhost:11434

Náº¿u tháº¥y "Ollama is running" â†’ OK

Náº¿u khÃ´ng, cháº¡y:

```bash
ollama serve
```

### BÆ°á»›c 2: Cháº¡y á»©ng dá»¥ng dá»‹ch thuáº­t

Má»Ÿ **Command Prompt má»›i** (náº¿u Ä‘ang dÃ¹ng venv, activate nÃ³):

```bash
python run_app.py
```

Káº¿t quáº£:
```
Running on local URL:  http://127.0.0.1:7860
```

### BÆ°á»›c 3: Má»Ÿ trÃ¬nh duyá»‡t

Truy cáº­p: http://localhost:7860

Báº¡n sáº½ tháº¥y giao diá»‡n Gradio vá»›i 2 Ã´:
- **Input**: DÃ¡n vÄƒn báº£n tiáº¿ng Anh
- **Outputs**: Stage 1 (dá»‹ch thÃ´) vÃ  Stage 2 (tinh chá»‰nh)

---

## ğŸ¯ Sá»­ dá»¥ng

1. DÃ¡n vÄƒn báº£n tiáº¿ng Anh vÃ o Ã´ **Input**
2. Nháº¥n **Translate**
3. Äá»£i vÃ i giÃ¢y (tÃ¹y Ä‘á»™ dÃ i vÄƒn báº£n)
4. Xem káº¿t quáº£ á»Ÿ 2 Ã´ **Stage 1 Output** vÃ  **Stage 2 Output**

**KhÃ´ng giá»›i háº¡n Ä‘á»™ dÃ i!** Báº¡n cÃ³ thá»ƒ dá»‹ch tá»« vÃ i cÃ¢u Ä‘áº¿n cáº£ bÃ i viáº¿t dÃ i.

---

## ğŸ› Xá»­ lÃ½ Lá»—i

### Lá»—i: "python is not recognized"

**NguyÃªn nhÃ¢n:** Python chÆ°a Ä‘Æ°á»£c thÃªm vÃ o PATH

**Giáº£i phÃ¡p:**
1. Gá»¡ cÃ i Ä‘áº·t Python
2. CÃ i láº¡i vÃ  **nhá»› tÃ­ch** "Add Python to PATH"
3. Restart Command Prompt

### Lá»—i: "No module named 'ctranslate2'"

**NguyÃªn nhÃ¢n:** ChÆ°a cÃ i Ä‘áº·t thÆ° viá»‡n

**Giáº£i phÃ¡p:**
```bash
pip install ctranslate2 transformers sentencepiece gradio requests torch
```

### Lá»—i: "Could not connect to Ollama"

**NguyÃªn nhÃ¢n:** Ollama server chÆ°a cháº¡y

**Giáº£i phÃ¡p:**
```bash
ollama serve
```

Hoáº·c kiá»ƒm tra System Tray (gÃ³c dÆ°á»›i cÃ¹ng bÃªn pháº£i) cÃ³ icon Ollama khÃ´ng

### Lá»—i: "Model not found: llama3.2:3b"

**NguyÃªn nhÃ¢n:** ChÆ°a táº£i mÃ´ hÃ¬nh

**Giáº£i phÃ¡p:**
```bash
ollama pull llama3.2:3b
```

### Lá»—i: "Timeout waiting for response"

**NguyÃªn nhÃ¢n:** VÄƒn báº£n quÃ¡ dÃ i hoáº·c Ollama cháº­m

**Giáº£i phÃ¡p:**

Chá»‰nh sá»­a `src/config/settings.json`:
```json
"timeout": 300
```

### Lá»—i: "CUDA out of memory"

**NguyÃªn nhÃ¢n:** GPU khÃ´ng Ä‘á»§ VRAM

**Giáº£i phÃ¡p:**
- Ollama tá»± Ä‘á»™ng fallback sang CPU (cháº­m hÆ¡n nhÆ°ng váº«n cháº¡y)
- Hoáº·c dÃ¹ng mÃ´ hÃ¬nh nhá» hÆ¡n: `ollama pull llama3.2:3b`

---

## ğŸ”§ NÃ¢ng cáº¥p GPU (TÃ¹y chá»n)

Náº¿u báº¡n cÃ³ GPU NVIDIA, cÃ i Ä‘áº·t CUDA Ä‘á»ƒ tÄƒng tá»‘c:

### BÆ°á»›c 1: CÃ i Ä‘áº·t CUDA Toolkit

Táº£i tá»«: https://developer.nvidia.com/cuda-downloads

Chá»n phiÃªn báº£n phÃ¹ há»£p vá»›i driver GPU cá»§a báº¡n.

### BÆ°á»›c 2: CÃ i PyTorch vá»›i CUDA

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

Thay `cu118` báº±ng phiÃªn báº£n CUDA cá»§a báº¡n (VD: `cu121` cho CUDA 12.1)

### BÆ°á»›c 3: Restart á»©ng dá»¥ng

```bash
python run_app.py
```

Ollama vÃ  CTranslate2 sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng GPU.

---

## ğŸ“Š Kiá»ƒm tra Hiá»‡u nÄƒng

### Kiá»ƒm tra GPU Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng

Má»Ÿ **Task Manager** (Ctrl + Shift + Esc) â†’ Tab **Performance** â†’ Chá»n GPU

Khi dá»‹ch, báº¡n sáº½ tháº¥y GPU Usage tÄƒng lÃªn â†’ GPU Ä‘ang Ä‘Æ°á»£c dÃ¹ng

### Benchmark tá»‘c Ä‘á»™

```python
import time
from pipeline_ollama import HybridTranslationPipeline

pipeline = HybridTranslationPipeline("config_ollama.json")

text = "Artificial intelligence is transforming how we interact with technology."

start = time.time()
stage1, stage2 = pipeline.translate(text)
end = time.time()

print(f"Time: {end - start:.2f}s")
```

---

## ğŸ†˜ Há»— trá»£ ThÃªm

### TÃ i liá»‡u

- **OLLAMA_GUIDE.md** - HÆ°á»›ng dáº«n chi tiáº¿t Ollama
- **QUICKSTART.md** - Báº¯t Ä‘áº§u nhanh trong 3 bÆ°á»›c
- **PROJECT_SUMMARY.md** - Tá»•ng quan kiáº¿n trÃºc

### Video hÆ°á»›ng dáº«n (náº¿u cÃ³)

[Link to video tutorials]

### Community

- GitHub Issues: [Link]
- Discord/Telegram: [Link]

---

## âœ… Checklist HoÃ n thÃ nh

Sau khi lÃ m theo hÆ°á»›ng dáº«n, báº¡n nÃªn cÃ³:

- âœ… Python 3.10+ cÃ i Ä‘áº·t vÃ  trong PATH
- âœ… Ollama cÃ i Ä‘áº·t vÃ  cháº¡y
- âœ… ThÆ° viá»‡n Python cÃ i Ä‘áº·t (ctranslate2, transformers, etc.)
- âœ… MÃ´ hÃ¬nh Stage 1 Ä‘Ã£ táº£i vÃ  chuyá»ƒn Ä‘á»•i (trong `models/opus-mt-en-vi-ct2/`)
- âœ… MÃ´ hÃ¬nh LLM Ä‘Ã£ táº£i (`ollama list` hiá»ƒn thá»‹ `llama3.2:3b`)
- âœ… á»¨ng dá»¥ng cháº¡y táº¡i http://localhost:7860
- âœ… CÃ³ thá»ƒ dá»‹ch vÄƒn báº£n khÃ´ng giá»›i háº¡n Ä‘á»™ dÃ i

---

**ğŸ‰ ChÃºc má»«ng! Báº¡n Ä‘Ã£ thiáº¿t láº­p thÃ nh cÃ´ng pipeline dá»‹ch thuáº­t trÃªn Windows!**

**BÆ°á»›c tiáº¿p theo**: Xem **OLLAMA_GUIDE.md** Ä‘á»ƒ tÃ¹y chá»‰nh vÃ  tá»‘i Æ°u pipeline.
